{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "tutorial_2_advanced_deep_learning_for_text.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "pOrLpn_G-ZMp",
        "_xyZikOT-ZMx",
        "-H5fFtGt-ZM0",
        "a8gF8TN1-ZM1",
        "C0GYy34n1zmB",
        "UM-2pRuUa5Os"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VENUSp4hdmSO"
      },
      "source": [
        "# Thinking with Deep Learning: Week 5 Part 2\n",
        "# Advanced Deep Learning for Text\n",
        "\n",
        "__Instructor:__ James Evans\n",
        "\n",
        "__Teaching Assistants & Content Creators/Organisers:__ Bhargav Srinivasa Desikan, Likun Cao & Partha Kadambi\n",
        "\n",
        "In this notebook, we will be exploring different state of the art deep learning models for text. Most of these methods are based on powerful encoder-decoder systems called Transformers, which use an alignment trick called Attention to model dependencies within inputs. We will also explore methods to better *understand* the workings of these models using interpretability techniques we introduced in Week 1. \n",
        "\n",
        "Throughout this tutorial, we will link to endorsed resources and blog posts that we highly recommend reading before diving into the code. The purpose of this notebook is to accustom you to the power of these large, pre-trained language models. We will explore most of the standard tasks one can do using these Transformers, as well as visualisations to better understand what makes a language model tick.\n",
        "\n",
        "NOTE: the first two sections of this tutorial introduce sequence to sequence (seq2seq) models, with and without attention. We use machine language translation between French and English as the example, but note that we can use any sequence to sequence pair (Question-Answer, Human-Chatbot, etc).\n",
        "\n",
        "NOTE: As usual, we advise you to check the HW first - we focus on testing your use of the transformers models, with an optional question (for extra credit) on sequence to sequence tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o90p-q8JNEMJ"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install shap\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGObI1Owu4yi"
      },
      "source": [
        "# Modelling Language and Sequences with Deep Neural Models\n",
        "\n",
        "Before we jump into the more popular, complex, deep learning models, it is worth going to the basics of language modelling Recurrent Neural Networks. \n",
        "\n",
        "These are two popular blog posts which take us through the structure and uses of RNNs.\n",
        "\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural\n",
        "   Network](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "   shows a bunch of real life examples\n",
        "-  [Understanding LSTM\n",
        "   Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "   is about LSTMs specifically but also informative about RNNs in\n",
        "   general."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpK94k4NvEbh"
      },
      "source": [
        "# empty cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIYsqan9y2Q0"
      },
      "source": [
        "## RNNs, LSTMs, GRUs\n",
        "\n",
        "Recurrent Neural Networks (RNN), Long Short Term Memory (LSTMs), Gated Recurrent Units (GRUs) are three neural network models which are especially suited for language modelling.\n",
        "\n",
        "We've seen in the last few weeks how these models can be used in different language based tasks, such as classification and embeddings. We will be seeing in this notebook how such models can be used for many many powerful tasks. \n",
        "\n",
        "Some of the earliest use of these models was text generation, which you would have seen in the blog posts we just linked to (and highly recommend reading). \n",
        "\n",
        "While we won't be testing you on these more basic models, we highly recommend checking out these GitHub repositories and tutorials with code for these models.\n",
        "\n",
        "- [PyTortch Using a chatacter level RNN](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)\n",
        "- [Keras LSTM charcter level text generation](https://keras.io/examples/generative/lstm_character_level_text_generation/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xApYGXkZ13kJ"
      },
      "source": [
        "# empty cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDTkQEaHNDe9"
      },
      "source": [
        "# Encoders, Decoders, Seq2Seq and Attention\n",
        "\n",
        "Encoder-Decoder models are a powerful way to deal with sequence to sequence problems - this kind of setup is also called seq2seq.\n",
        "\n",
        "When we use this kind of sequence to sequence alignment and also map certain parts of the sequences with different values (a process called Attention), the sequence to sequence prediction task performance increases. In this section and the next we will focus on Machine Translation as the domain to try Encoder and Decoder tasks, without and then with attention. \n",
        "\n",
        "**We highly recommend using this [visual blog](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) which explains seq2seq and attention based models**\n",
        "\n",
        "We will be using the code from a Keras tutorial and PyTorch tutorial to demonstrate these models.\n",
        "\n",
        "NOTE: these sections can be skimmed through - you can go through these in detail if you decide to do the HW question on sequence to sequence pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9UQ-9budmSS"
      },
      "source": [
        "# empty cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIuje1-Ve-67"
      },
      "source": [
        "## Character-level recurrent sequence-to-sequence model\n",
        "\n",
        "This code is adapted from the [Keras official tutorial](https://keras.io/examples/nlp/lstm_seq2seq/) on seq2seq using LSTM, authored by [fchollet](https://twitter.com/fchollet). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW7DdSj0e-7J"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "This example demonstrates how to implement a basic character-level\n",
        "recurrent sequence-to-sequence model. We apply it to translating\n",
        "short English sentences into short French sentences,\n",
        "character-by-character. Note that it is fairly unusual to\n",
        "do character-level machine translation, as word-level\n",
        "models are more common in this domain.\n",
        "\n",
        "**Summary of the algorithm**\n",
        "\n",
        "- We start with input sequences from a domain (e.g. English sentences)\n",
        "    and corresponding target sequences from another domain\n",
        "    (e.g. French sentences).\n",
        "- An encoder LSTM turns input sequences to 2 state vectors\n",
        "    (we keep the last LSTM state and discard the outputs).\n",
        "- A decoder LSTM is trained to turn the target sequences into\n",
        "    the same sequence but offset by one timestep in the future,\n",
        "    a training process called \"teacher forcing\" in this context.\n",
        "    It uses as initial state the state vectors from the encoder.\n",
        "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
        "    given `targets[...t]`, conditioned on the input sequence.\n",
        "- In inference mode, when we want to decode unknown input sequences, we:\n",
        "    - Encode the input sequence into state vectors\n",
        "    - Start with a target sequence of size 1\n",
        "        (just the start-of-sequence character)\n",
        "    - Feed the state vectors and 1-char target sequence\n",
        "        to the decoder to produce predictions for the next character\n",
        "    - Sample the next character using these predictions\n",
        "        (we simply use argmax).\n",
        "    - Append the sampled character to the target sequence\n",
        "    - Repeat until we generate the end-of-sequence character or we\n",
        "        hit the character limit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk8__4jEe-7L"
      },
      "source": [
        "### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjnD7Gtle-7M"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9axnF-ie-7O"
      },
      "source": [
        "### Download the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik7TrAiMe-7P"
      },
      "source": [
        "!!curl -O http://www.manythings.org/anki/fra-eng.zip\n",
        "!!unzip fra-eng.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzpQ-1sre-7T"
      },
      "source": [
        "### Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14m6qbZVe-7T"
      },
      "source": [
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = \"fra.txt\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ1NzQs6e-7U"
      },
      "source": [
        "### Prepare the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VXgK_P4e-7V"
      },
      "source": [
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmT-s5TGe-7X"
      },
      "source": [
        "### Build the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1emHhx2e-7Y"
      },
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KVvUL8re-7b"
      },
      "source": [
        "### Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIXh70Mte-7c"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2,\n",
        ")\n",
        "# Save model\n",
        "model.save(\"s2s\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HgC-DTCe-7d"
      },
      "source": [
        "### Run inference (sampling)\n",
        "\n",
        "1. encode input and retrieve initial decoder state\n",
        "2. run one step of decoder with this initial state\n",
        "and a \"start of sequence\" token as target.\n",
        "Output will be the next target token.\n",
        "3. Repeat with the current target token and current states\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JSbU7pse-7f"
      },
      "source": [
        "# Define sampling models\n",
        "# Restore the model and construct the encoder and decoder.\n",
        "model = keras.models.load_model(\"s2s\")\n",
        "\n",
        "encoder_inputs = model.input[0]  # input_1\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_inputs = model.input[1]  # input_2\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"input_3\")\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"input_4\")\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm = model.layers[3]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[4]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = keras.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8K7BU-fe-7i"
      },
      "source": [
        "You can now generate decoded sentences as such:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHNiUUmPe-7j"
      },
      "source": [
        "for seq_index in range(20):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print(\"-\")\n",
        "    print(\"Input sentence:\", input_texts[seq_index])\n",
        "    print(\"Decoded sentence:\", decoded_sentence)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTFsrjOgoTjB"
      },
      "source": [
        "\n",
        "##  Translation with a Sequence to Sequence Network and Attention\n",
        "\n",
        "This code is adapted from the [PyTorch official tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html), authored by [Sean Robertson](https://github.com/spro/practical-pytorch).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03fSZ2pmv02A"
      },
      "source": [
        "### Introduction\n",
        "In this project we will be teaching a neural network to translate from\n",
        "French to English.\n",
        "\n",
        "\n",
        "    [KEY: > input, = target, < output]\n",
        "\n",
        "    > il est en train de peindre un tableau .\n",
        "    = he is painting a picture .\n",
        "    < he is painting a picture .\n",
        "\n",
        "    > pourquoi ne pas essayer ce vin delicieux ?\n",
        "    = why not try that delicious wine ?\n",
        "    < why not try that delicious wine ?\n",
        "\n",
        "    > elle n est pas poete mais romanciere .\n",
        "    = she is not a poet but a novelist .\n",
        "    < she not not a poet but a novelist .\n",
        "\n",
        "    > vous etes trop maigre .\n",
        "    = you re too skinny .\n",
        "    < you re all alone .\n",
        "\n",
        "... to varying degrees of success.\n",
        "\n",
        "This is made possible by the simple but powerful idea of the [sequence\n",
        "to sequence network](https://arxiv.org/abs/1409.3215), in which two\n",
        "recurrent neural networks work together to transform one sequence to\n",
        "another. An encoder network condenses an input sequence into a vector,\n",
        "and a decoder network unfolds that vector into a new sequence.\n",
        "\n",
        "\n",
        "To improve upon this model we'll use an [attention\n",
        "mechanism](https://arxiv.org/abs/1409.0473), which lets the decoder\n",
        "learn to focus over a specific range of the input sequence.\n",
        "\n",
        "### imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIHXqUQvoTjD"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xk9k0xgoTjE"
      },
      "source": [
        "### Loading data files\n",
        "\n",
        "\n",
        "The data for this project is a set of many thousands of English to\n",
        "French translation pairs.\n",
        "\n",
        "`This question on Open Data Stack\n",
        "Exchange <https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages>`__\n",
        "pointed me to the open translation site https://tatoeba.org/ which has\n",
        "downloads available at https://tatoeba.org/eng/downloads - and better\n",
        "yet, someone did the extra work of splitting language pairs into\n",
        "individual text files here: https://www.manythings.org/anki/\n",
        "\n",
        "The English to French pairs are too big to include in the repo, so\n",
        "download to ``data/eng-fra.txt`` before continuing. The file is a tab\n",
        "separated list of translation pairs:\n",
        "\n",
        "::\n",
        "\n",
        "    I am cold.    J'ai froid.\n",
        "\n",
        ".. Note::\n",
        "   Download the data from\n",
        "   `here <https://download.pytorch.org/tutorial/data.zip>`_\n",
        "   and extract it to the current directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8VYnzaHoTjE"
      },
      "source": [
        "Similar to the character encoding used in the character-level RNN\n",
        "tutorials, we will be representing each word in a language as a one-hot\n",
        "vector, or giant vector of zeros except for a single one (at the index\n",
        "of the word). Compared to the dozens of characters that might exist in a\n",
        "language, there are many many more words, so the encoding vector is much\n",
        "larger. We will however cheat a bit and trim the data to only use a few\n",
        "thousand words per language.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/word-encoding.png\n",
        "   :alt:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUB-q_f8oTjG"
      },
      "source": [
        "We'll need a unique index per word to use as the inputs and targets of\n",
        "the networks later. To keep track of all this we will use a helper class\n",
        "called ``Lang`` which has word → index (``word2index``) and index → word\n",
        "(``index2word``) dictionaries, as well as a count of each word\n",
        "``word2count`` to use to later replace rare words.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MIiriTeLx69o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yulUXQbioTjG"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBwz8kK8oTjH"
      },
      "source": [
        "The files are all in Unicode, to simplify we will turn Unicode\n",
        "characters to ASCII, make everything lowercase, and trim most\n",
        "punctuation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiyZRpvFoTjH"
      },
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2iz97nYoTjI"
      },
      "source": [
        "To read the data file we will split the file into lines, and then split\n",
        "lines into pairs. The files are all English → Other Language, so if we\n",
        "want to translate from Other Language → English I added the ``reverse``\n",
        "flag to reverse the pairs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8gLtSLyoTjI"
      },
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('/content/drive/MyDrive/deep-learning/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90NyCjp7oTjJ"
      },
      "source": [
        "Since there are a *lot* of example sentences and we want to train\n",
        "something quickly, we'll trim the data set to only relatively short and\n",
        "simple sentences. Here the maximum length is 10 words (that includes\n",
        "ending punctuation) and we're filtering to sentences that translate to\n",
        "the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced\n",
        "earlier).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hC5odnfoTjJ"
      },
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFJFSQrvoTjK"
      },
      "source": [
        "The full process for preparing the data is:\n",
        "\n",
        "-  Read text file and split into lines, split lines into pairs\n",
        "-  Normalize text, filter by length and content\n",
        "-  Make word lists from sentences in pairs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zbqOWRVoTjK"
      },
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KElEyvZOoTjL"
      },
      "source": [
        "### The Seq2Seq Model\n",
        "\n",
        "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
        "sequence and uses its own output as input for subsequent steps.\n",
        "\n",
        "A `Sequence to Sequence network <https://arxiv.org/abs/1409.3215>`__, or\n",
        "seq2seq network, or `Encoder Decoder\n",
        "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
        "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
        "an input sequence and outputs a single vector, and the decoder reads\n",
        "that vector to produce an output sequence.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
        "   :alt:\n",
        "\n",
        "Unlike sequence prediction with a single RNN, where every input\n",
        "corresponds to an output, the seq2seq model frees us from sequence\n",
        "length and order, which makes it ideal for translation between two\n",
        "languages.\n",
        "\n",
        "Consider the sentence \"Je ne suis pas le chat noir\" → \"I am not the\n",
        "black cat\". Most of the words in the input sentence have a direct\n",
        "translation in the output sentence, but are in slightly different\n",
        "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
        "construction there is also one more word in the input sentence. It would\n",
        "be difficult to produce a correct translation directly from the sequence\n",
        "of input words.\n",
        "\n",
        "With a seq2seq model the encoder creates a single vector which, in the\n",
        "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
        "vector — a single point in some N dimensional space of sentences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRgxRkttoTjM"
      },
      "source": [
        "### The Encoder\n",
        "\n",
        "The encoder of a seq2seq network is a RNN that outputs some value for\n",
        "every word from the input sentence. For every input word the encoder\n",
        "outputs a vector and a hidden state, and uses the hidden state for the\n",
        "next input word.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/encoder-network.png\n",
        "   :alt:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcpRP9DKoTjM"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koRIOArfoTjM"
      },
      "source": [
        "### The Decoder\n",
        "\n",
        "The decoder is another RNN that takes the encoder output vector(s) and\n",
        "outputs a sequence of words to create the translation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUT9JS-poTjN"
      },
      "source": [
        "\n",
        "\n",
        "In the simplest seq2seq decoder we use only last output of the encoder.\n",
        "This last output is sometimes called the *context vector* as it encodes\n",
        "context from the entire sequence. This context vector is used as the\n",
        "initial hidden state of the decoder.\n",
        "\n",
        "At every step of decoding, the decoder is given an input token and\n",
        "hidden state. The initial input token is the start-of-string ``<SOS>``\n",
        "token, and the first hidden state is the context vector (the encoder's\n",
        "last hidden state).\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/decoder-network.png\n",
        "   :alt:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJoM8Mn4oTjN"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLkleT6hoTjO"
      },
      "source": [
        "I encourage you to train and observe the results of this model, but to\n",
        "save space we'll be going straight for the gold and introducing the\n",
        "Attention Mechanism.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCR7DceXoTjO"
      },
      "source": [
        "Attention Decoder\n",
        "^^^^^^^^^^^^^^^^^\n",
        "\n",
        "If only the context vector is passed betweeen the encoder and decoder,\n",
        "that single vector carries the burden of encoding the entire sentence.\n",
        "\n",
        "Attention allows the decoder network to \"focus\" on a different part of\n",
        "the encoder's outputs for every step of the decoder's own outputs. First\n",
        "we calculate a set of *attention weights*. These will be multiplied by\n",
        "the encoder output vectors to create a weighted combination. The result\n",
        "(called ``attn_applied`` in the code) should contain information about\n",
        "that specific part of the input sequence, and thus help the decoder\n",
        "choose the right output words.\n",
        "\n",
        ".. figure:: https://i.imgur.com/1152PYf.png\n",
        "   :alt:\n",
        "\n",
        "Calculating the attention weights is done with another feed-forward\n",
        "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
        "Because there are sentences of all sizes in the training data, to\n",
        "actually create and train this layer we have to choose a maximum\n",
        "sentence length (input length, for encoder outputs) that it can apply\n",
        "to. Sentences of the maximum length will use all the attention weights,\n",
        "while shorter sentences will only use the first few.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/attention-decoder-network.png\n",
        "   :alt:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-tZt8K8oTjO"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrQR6uRVoTjP"
      },
      "source": [
        "\n",
        "\n",
        "### Training\n",
        "\n",
        "Preparing Training Data\n",
        "\n",
        "To train, for each pair we will need an input tensor (indexes of the\n",
        "words in the input sentence) and target tensor (indexes of the words in\n",
        "the target sentence). While creating these vectors we will append the\n",
        "EOS token to both sequences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPDyE3u6oTjP"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq6IR_LgoTjP"
      },
      "source": [
        "### Training the Model\n",
        "\n",
        "To train we run the input sentence through the encoder, and keep track\n",
        "of every output and the latest hidden state. Then the decoder is given\n",
        "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
        "encoder as its first hidden state.\n",
        "\n",
        "\"Teacher forcing\" is the concept of using the real target outputs as\n",
        "each next input, instead of using the decoder's guess as the next input.\n",
        "Using teacher forcing causes it to converge faster but `when the trained\n",
        "network is exploited, it may exhibit\n",
        "instability <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf>`__.\n",
        "\n",
        "You can observe outputs of teacher-forced networks that read with\n",
        "coherent grammar but wander far from the correct translation -\n",
        "intuitively it has learned to represent the output grammar and can \"pick\n",
        "up\" the meaning once the teacher tells it the first few words, but it\n",
        "has not properly learned how to create the sentence from the translation\n",
        "in the first place.\n",
        "\n",
        "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
        "choose to use teacher forcing or not with a simple if statement. Turn\n",
        "``teacher_forcing_ratio`` up to use more of it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X76FoDFoTjP"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhBofIiGoTjQ"
      },
      "source": [
        "This is a helper function to print time elapsed and estimated time\n",
        "remaining given the current time and progress %.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cbdMCvNoTjQ"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mklP0f8VoTjQ"
      },
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "-  Start a timer\n",
        "-  Initialize optimizers and criterion\n",
        "-  Create set of training pairs\n",
        "-  Start empty losses array for plotting\n",
        "\n",
        "Then we call ``train`` many times and occasionally print the progress (%\n",
        "of examples, time so far, estimated time) and average loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFA8uyDBoTjQ"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slMtEO_FoTjR"
      },
      "source": [
        "### Plotting results\n",
        "\n",
        "Plotting is done with matplotlib, using the array of loss values\n",
        "``plot_losses`` saved while training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-hQtgY9oTjR"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8wE-4jioTjR"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Evaluation is mostly the same as training, but there are no targets so\n",
        "we simply feed the decoder's predictions back to itself for each step.\n",
        "Every time it predicts a word we add it to the output string, and if it\n",
        "predicts the EOS token we stop there. We also store the decoder's\n",
        "attention outputs for display later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_khje6GoTjR"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV5vpawIoTjR"
      },
      "source": [
        "We can evaluate random sentences from the training set and print out the\n",
        "input, target, and output to make some subjective quality judgements:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLDbHlxmoTjS"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u_pJpTKoTjS"
      },
      "source": [
        "### Training and Evaluating\n",
        "\n",
        "With all these helper functions in place (it looks like extra work, but\n",
        "it makes it easier to run multiple experiments) we can actually\n",
        "initialize a network and start training.\n",
        "\n",
        "Remember that the input sentences were heavily filtered. For this small\n",
        "dataset we can use relatively small networks of 256 hidden nodes and a\n",
        "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
        "reasonable results.\n",
        "\n",
        ".. Note::\n",
        "   If you run this notebook you can train, interrupt the kernel,\n",
        "   evaluate, and continue training later. Comment out the lines where the\n",
        "   encoder and decoder are initialized and run ``trainIters`` again.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9HTlKHcoTjS"
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUMOAFIHoTjS",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TLHkNqzoTjS",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Visualizing Attention\n",
        "\n",
        "\n",
        "A useful property of the attention mechanism is its highly interpretable\n",
        "outputs. Because it is used to weight specific encoder outputs of the\n",
        "input sequence, we can imagine looking where the network is focused most\n",
        "at each time step.\n",
        "\n",
        "You could simply run ``plt.matshow(attentions)`` to see attention output\n",
        "displayed as a matrix, with the columns being input steps and rows being\n",
        "output steps:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGhWjlvXoTjT"
      },
      "source": [
        "output_words, attentions = evaluate(\n",
        "    encoder1, attn_decoder1, \"je suis trop froid .\")\n",
        "plt.matshow(attentions.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzJbc42eoTjT"
      },
      "source": [
        "For a better viewing experience we will do the extra work of adding axes\n",
        "and labels:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZlEiTYdoTjT"
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)\n",
        "\n",
        "\n",
        "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
        "\n",
        "evaluateAndShowAttention(\"elle est trop petit .\")\n",
        "\n",
        "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
        "\n",
        "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOXhlS5ANBDx"
      },
      "source": [
        "# Transformers\n",
        "\n",
        "Transformers are a neural architecture comprised of encoder component and a decoder component, which use self-attention to reach high levels of performance for sequence to sequence tasks (and drops all RNN components!)\n",
        "\n",
        "**We highly recommend reading this [visual blog](http://jalammar.github.io/illustrated-transformer/) on the Transformer before proceeding.**\n",
        "\n",
        "\n",
        "## Models\n",
        "\n",
        "We usually see two families of models - the Generative Pre-Trained Transformer models (GPT-n) developed by Open-AI, and the BERT family of models developed by Google, which featured Bi-directional Transformers as language models.\n",
        "\n",
        "### GPT and similar models\n",
        "\n",
        "These are auto-regressive language models created by [OpenAI](https://openai.com/). GPT3 has 175 billion parameters and is one of the largest, most powerful generative language model available today.\n",
        "\n",
        "**We highly recommend reading this [visual blog](http://jalammar.github.io/illustrated-gpt2/) on the GPT-2 before proceeding.**\n",
        "\n",
        "\n",
        "### BERT and similar models\n",
        "\n",
        "The BERT model, first presented by Google Research, uses bi-directional transformers and two tasks - Masked Language Modelling task, and Next Sentence Prediction task, to train the model. These large transformers based models are trained on lots of data (Wikipedia + books). BERT has since spun off many models, include language specific ones (SpanBERT), domain specific models (SciBERT), and different level of sequences (CharacterBERT). \n",
        "\n",
        "**We highly recommend reading this [visual blog](http://jalammar.github.io/illustrated-bert/) on BERT and similar models before proceeding**.\n",
        "\n",
        "Since BERT, the state of the art has constatly been beat by models such as XL-Net. We recommend this [blog post](https://medium.com/@phylypo/a-survey-of-the-state-of-the-art-language-models-up-to-early-2020-aba824302c6) which describes various language models as of 2020 (right before GPT-3 burst on the scene). \n",
        "\n",
        "We also highly recommend these papers for a more critical view on such language models:\n",
        "\n",
        "[On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜](https://dl.acm.org/doi/10.1145/3442188.3445922) - Bender et al, 2021\n",
        "\n",
        "[Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data](https://www.aclweb.org/anthology/2020.acl-main.463/) - Bender and Koller, 2020\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX2nO2EaAdHf"
      },
      "source": [
        "# empty cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cJ-SkJpAdhp"
      },
      "source": [
        "## Popular Tasks using Transformers & Model Explanations\n",
        "\n",
        "We will now use the 'pipelines' feature from the popular Transformers package to explore common NLP tasks. For several of these tasks, we perform model interpretability exercises that provide a window into model's decision-making process by computing the SHAP values of different parts of our input. Before we jump into our code, let's take a brief overview of the concepts behind our model explanations.\n",
        "\n",
        "SHAP ([SHapley Additive exPlanations](https://arxiv.org/abs/1705.07874)) - which we introduced in Week 1 - is a game theoretic approach to explain model outputs based on [Shapley values](https://christophm.github.io/interpretable-ml-book/shapley.html). The intuition behind Shapley values is simple: It treats model inputs as a 'coalition' of features ('players') that collectively contribute to the output ('reward'). It then estimates the importance of any particular feature as the average marginal contribution of the feature across all combinatorial possibilities of 'coalitions' of features.\n",
        "\n",
        "[SHAP](https://christophm.github.io/interpretable-ml-book/shap.html#shap) is a computational approach to estimating Shapley values by building a separate 'explanation' model to explain the output of any particular input. It is a model agnostic measure (it is not specific to any particular model architecture), making it a very handy analytical tool to better understand and compare the deep-learning-based models we build.\n",
        "\n",
        "[Transformers Documentation](https://huggingface.co/transformers/)\n",
        "\n",
        "[Transformers GitHub](https://github.com/huggingface/transformers)\n",
        "\n",
        "[A great resource for interpretable ML by Christoph Molnar](https://christophm.github.io/interpretable-ml-book/)\n",
        "\n",
        "The following sections of code are based on the [Summary of Tasks](https://huggingface.co/transformers/task_summary.html) page in the Transformers documentation and [SHAP documentation.](https://shap.readthedocs.io/en/latest/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequence Classification\n",
        "\n",
        "Here, we look at an example of emotion identification: Classifying text into joy, sadness, love, anger, fear, or surprise."
      ],
      "metadata": {
        "id": "oRcgtG9MqHi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import transformers\n",
        "import datasets\n",
        "import shap\n",
        "\n",
        "# load the emotion dataset\n",
        "dataset  = datasets.load_dataset(\"emotion\", split = \"train\")\n",
        "data = pd.DataFrame({'text':dataset['text'],'emotion':dataset['label']})"
      ],
      "metadata": {
        "id": "zuLXZ8VsqSCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model and tokenizer\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"nateraw/bert-base-uncased-emotion\", use_fast=True)\n",
        "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"nateraw/bert-base-uncased-emotion\").cuda()\n",
        "\n",
        "# build a pipeline object to do predictions\n",
        "pred = transformers.pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0, return_all_scores=True)"
      ],
      "metadata": {
        "id": "wxZ3SO3Uqp6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text'][:3]"
      ],
      "metadata": {
        "id": "mTPjxQqiCAZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's run a single example through our pipeline:"
      ],
      "metadata": {
        "id": "nupRdlAVE00w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eg = \"Felt so intimidated by all that code!\""
      ],
      "metadata": {
        "id": "Q8xaobgJE9uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred(eg)"
      ],
      "metadata": {
        "id": "9M68nMazEnLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get softmax values over our labels.\n",
        "That output makes sense! \n",
        "\n",
        "Let's now *explain* the reasoning of ou model:"
      ],
      "metadata": {
        "id": "MKjXK2jyFY4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creaing an explainer that wraps around our pipeline\n",
        "explainer = shap.Explainer(pred)"
      ],
      "metadata": {
        "id": "WNfQzzSOqp9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code block below, we visualize the explainer we built on our three text inputs. \n",
        "\n",
        "For each example at the top, we see can our five emotion labels, with fear highlighted in red for our first example to indicate that it is the prediction for the input. If you click on 'fear' the graphic changes to indicate which words in our input has the most positive SHAP contribution (red) and negative contribution (blue) to our final decision. To be clear, both red and blue features matter to the model: red positively influences label probability; blue negatively influences label probability. The width of the bar (right coordinate - left coordinate) for each token indicates the magnitude of our estimate. You might see that many featurees have very litte corresponding width - this indicates that these features have negligible influence on label probabilites (for the input in question).\n",
        "\n",
        "In the case below, we see that 'intimidated' is a strong red - which makes a lot of sense!"
      ],
      "metadata": {
        "id": "1t6gtFU0CWOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Estimating SHAP values for our example\n",
        "shap_values = explainer([eg])"
      ],
      "metadata": {
        "id": "yCvGyktvF5nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.text(shap_values)"
      ],
      "metadata": {
        "id": "H81pzPAIGH56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A cool feature of the explainer is that you can click on the other labels as well - not just the final prediction - to understand what the model 'thinks' of each label and input! \n",
        "\n",
        "Play around with some additional examples below - you should see that our model is building a lot of dependencies behind the scenes (thanks to attention): "
      ],
      "metadata": {
        "id": "fpwQe-jUGuuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Estimating SHAP values for first three text inputs\n",
        "shap_values = explainer(data['text'][:3])"
      ],
      "metadata": {
        "id": "mv_oi29jqqAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.text(shap_values)"
      ],
      "metadata": {
        "id": "Zor92vW4JCW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also limit your visualization to focus on explanations for a particular label:"
      ],
      "metadata": {
        "id": "DtL8jsd4IzXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.text(shap_values[:, :, \"anger\"])"
      ],
      "metadata": {
        "id": "FIduYlugqw23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use other plots to help us compare feature contributions more readily. Below, we calculate feature importance values across *all* of our three prior inputs. This is a powerful tool to directly investigate the associations our model builds between labels and data."
      ],
      "metadata": {
        "id": "d_F7Cyh_JJFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.bar(shap_values[:,:,\"joy\"].mean(0))"
      ],
      "metadata": {
        "id": "K4QVORp4qw50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "rinting features with most negative relationship with label:"
      ],
      "metadata": {
        "id": "v6qEkAQPK0yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.bar(shap_values[:,:,\"joy\"].mean(0), order=shap.Explanation.argsort)"
      ],
      "metadata": {
        "id": "u1S8KDTOqw8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the most positive relationships with label:"
      ],
      "metadata": {
        "id": "girmIHWXK-2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ...or acending order\n",
        "shap.plots.bar(shap_values[:,:,\"joy\"].mean(0), order=shap.Explanation.argsort.flip)"
      ],
      "metadata": {
        "id": "g8kjiBAoqw_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2oChnp1-ZMm"
      },
      "source": [
        "### Extractive Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12uRPuEq-ZMm"
      },
      "source": [
        "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
        "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune a\n",
        "model on a SQuAD task, you may leverage the [run_squad.py](https://github.com/huggingface/transformers/tree/master/examples/question-answering/run_squad.py) and\n",
        "[run_tf_squad.py](https://github.com/huggingface/transformers/tree/master/examples/question-answering/run_tf_squad.py) scripts.\n",
        "\n",
        "\n",
        "Here is an example of using pipelines to do question answering: extracting an answer from a text given a question. It\n",
        "leverages a fine-tuned model on SQuAD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ast1vLPT-ZMm"
      },
      "source": [
        "from transformers import pipeline\n",
        "nlp = pipeline(\"question-answering\")\n",
        "context = r\"\"\"\n",
        "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
        "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
        "a model on a SQuAD task, you may leverage the examples/question-answering/run_squad.py script.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5fq8iB0-ZMn"
      },
      "source": [
        "This returns an answer extracted from the text, a confidence score, alongside \"start\" and \"end\" values, which are the\n",
        "positions of the extracted answer in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMSCEtkU-ZMn"
      },
      "source": [
        "result = nlp(question=\"What is extractive question answering?\", context=context)\n",
        "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
        "result = nlp(question=\"What is a good example of a question answering dataset?\", context=context)\n",
        "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now take a look at model explanations for extractive question answering:"
      ],
      "metadata": {
        "id": "8xGv8frNMLZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap"
      ],
      "metadata": {
        "id": "fvL0tl8zmcz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(questions, start):\n",
        "    outs = []\n",
        "    for q in questions:\n",
        "        question, context = q.split(\"[SEP]\")\n",
        "        d = nlp.tokenizer(question, context)\n",
        "        out = nlp.model.forward(**{k: torch.tensor(d[k]).reshape(1, -1) for k in d})\n",
        "        logits = out.start_logits if start else out.end_logits\n",
        "        outs.append(logits.reshape(-1).detach().numpy())\n",
        "    return outs\n",
        "def f_start(questions):\n",
        "    return f(questions, True)\n",
        "def f_end(questions):\n",
        "    return f(questions, False)\n",
        "\n",
        "# attach a dynamic output_names property to the models so we can plot the tokens at each output position\n",
        "def out_names(inputs):\n",
        "    question, context = inputs.split(\"[SEP]\")\n",
        "    d = nlp.tokenizer(question, context)\n",
        "    return [nlp.tokenizer.decode([id]) for id in d[\"input_ids\"]]\n",
        "f_start.output_names = out_names\n",
        "f_end.output_names = out_names"
      ],
      "metadata": {
        "id": "JeARlTObmsrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first take a look at model explanations for identifying the start of our answer:"
      ],
      "metadata": {
        "id": "A8tXRxy4PaoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\"\"\"What is extractive question answering?[SEP]Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
        "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
        "a model on a SQuAD task, you may leverage the examples/question-answering/run_squad.py script.\"\"\"]\n",
        "\n",
        "explainer_start = shap.Explainer(f_start, nlp.tokenizer)\n",
        "shap_values_start = explainer_start(data)\n",
        "\n",
        "shap.plots.text(shap_values_start)"
      ],
      "metadata": {
        "id": "UJ-fwigzmPqA",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now take a look at model explanations for identifying the end of our answer:"
      ],
      "metadata": {
        "id": "AdFC3kvEPk67",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explainer_end = shap.Explainer(f_end, nlp.tokenizer)\n",
        "shap_values_end = explainer_end(data)\n",
        "\n",
        "shap.plots.text(shap_values_end)"
      ],
      "metadata": {
        "id": "CL_umzIfoBeW",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally, the whole extract. Click on the alternative answers and notice how the model is paying 'attention' to different parts of our context:\n"
      ],
      "metadata": {
        "id": "4XHTFBFIPpbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_answer_scorer(answers):\n",
        "    def f(questions):\n",
        "        out = []\n",
        "        for q in questions:\n",
        "            question, context = q.split(\"[SEP]\")\n",
        "            results = nlp(question, context, top_k=20)\n",
        "            values = []\n",
        "            for answer in answers:\n",
        "                value = 0\n",
        "                for result in results:\n",
        "                    if result[\"answer\"] == answer:\n",
        "                        value = result[\"score\"]\n",
        "                        break\n",
        "                values.append(value)\n",
        "            out.append(values)\n",
        "        return out\n",
        "    f.output_names = answers\n",
        "    return f\n",
        "\n",
        "f_answers = make_answer_scorer([\"the task of extracting an answer from a text given a question\", \"SQuAD dataset\", \"run_squad.py script\"])\n",
        "explainer_answers = shap.Explainer(f_answers, nlp.tokenizer)\n",
        "shap_values_answers = explainer_answers(data)"
      ],
      "metadata": {
        "id": "X6WWR3kHoFPe",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "shap.plots.text(shap_values_answers)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "wQCnb3bnvZhm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t3RxO8D-ZMn"
      },
      "source": [
        "We instantiated a pipeline automatically using the Transformers library in the previous example. Here is another example of question answering using a model and a tokenizer. The process is the following:\n",
        "\n",
        "1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a BERT model and loads it\n",
        "   with the weights stored in the checkpoint.\n",
        "2. Define a text and a few questions.\n",
        "3. Iterate over the questions and build a sequence from the text and the current question, with the correct\n",
        "   model-specific separators token type ids and attention masks.\n",
        "4. Pass this sequence through the model. This outputs a range of scores across the entire sequence of tokens (question and\n",
        "   text), for both the start and end positions.\n",
        "5. Compute the softmax of the result to get probabilities over the tokens.\n",
        "6. Fetch the tokens from the identified start and stop values, convert those tokens to a string.\n",
        "7. Print the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbvf9egg-ZMo"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "text = r\"\"\"\n",
        "🤗 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
        "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural\n",
        "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
        "TensorFlow 2.0 and PyTorch.\n",
        "\"\"\"\n",
        "questions = [\n",
        "    \"How many pretrained models are available in 🤗 Transformers?\",\n",
        "    \"What does 🤗 Transformers provide?\",\n",
        "    \"🤗 Transformers provides interoperability between which frameworks?\",\n",
        "]\n",
        "for question in questions:\n",
        "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    outputs = model(**inputs)\n",
        "    answer_start_scores = outputs.start_logits\n",
        "    answer_end_scores = outputs.end_logits\n",
        "\n",
        "    answer_start = torch.argmax(\n",
        "        answer_start_scores\n",
        "    )  # Get the most likely beginning of answer with the argmax of the score\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
        "\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOrLpn_G-ZMp"
      },
      "source": [
        "### Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AEzL3nu-ZMp"
      },
      "source": [
        "Language modeling is the task of fitting a model to a corpus, which can be domain specific. All popular\n",
        "transformer-based models are trained using a variant of language modeling, e.g., BERT with masked language modeling,\n",
        "GPT-2/3 with causal language modeling.\n",
        "\n",
        "Language modeling can be useful beyond pretraining as well, for example to shift the model distribution to be\n",
        "domain-specific: using a language model trained over a very large corpus, and then fine-tuning it to a news dataset or\n",
        "on scientific papers e.g. [LysandreJik/arxiv-nlp](https://huggingface.co/lysandre/arxiv-nlp)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zwgOcdZ-ZMp"
      },
      "source": [
        "#### Masked Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r5bR8hM-ZMp"
      },
      "source": [
        "Masked language modeling is the task of masking tokens in a sequence with a masking token, and prompting the model to\n",
        "fill that mask with an appropriate token. This allows the model to attend to both the right context (tokens on the\n",
        "right of the mask) and the left context (tokens on the left of the mask). Such a training creates a strong basis for\n",
        "downstream tasks requiring bi-directional context, such as SQuAD (question answering, see [Lewis, Lui, Goyal et al.](https://arxiv.org/abs/1910.13461), part 4.2).\n",
        "\n",
        "Here is an example of using pipelines to replace a mask from a sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBTkRCoE-ZMq"
      },
      "source": [
        "from transformers import pipeline\n",
        "nlp = pipeline(\"fill-mask\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDiURWsT-ZMq"
      },
      "source": [
        "This outputs the sequences with the mask filled, the confidence score, and the token id in the tokenizer vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayEq2LE2-ZMq"
      },
      "source": [
        "from pprint import pprint\n",
        "pprint(nlp(f\"HuggingFace is creating a {nlp.tokenizer.mask_token} that the community uses to solve NLP tasks.\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUN9TACu-ZMq"
      },
      "source": [
        "Here is an example of doing masked language modeling using a model and a tokenizer. The process as follows:\n",
        "\n",
        "1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a DistilBERT model and\n",
        "   loads it with the weights stored in the checkpoint.\n",
        "2. Define a sequence with a masked token, placing the `tokenizer.mask_token` instead of a word.\n",
        "3. Encode that sequence into a list of IDs and find the position of the masked token in that list.\n",
        "4. Retrieve the predictions at the index of the mask token: this tensor has the same size as the vocabulary, and the\n",
        "   values are the scores attributed to each token. The model gives higher scores to tokens it deems probable in that\n",
        "   context.\n",
        "5. Retrieve the top 5 tokens using the PyTorch `topk` or TensorFlow `top_k` methods.\n",
        "6. Replace the mask token by the tokens and print the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihYScqSt-ZMq"
      },
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "import torch\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"distilbert-base-cased\")\n",
        "sequence = f\"Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
        "input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
        "token_logits = model(input).logits\n",
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVBAG5Al-ZMr"
      },
      "source": [
        "This prints five sequences, with the top 5 tokens predicted by the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyBSdXGp-ZMr"
      },
      "source": [
        "for token in top_5_tokens:\n",
        "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxGiKa1h-ZMr"
      },
      "source": [
        "#### Causal Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt7FighC-ZMr"
      },
      "source": [
        "Causal language modeling is the task of predicting the token following a sequence of tokens. In this situation, the\n",
        "model only attends to the left context (tokens on the left of the mask). Such a training is particularly interesting\n",
        "for generation tasks.\n",
        "\n",
        "Usually, the next token is predicted by sampling from the logits of the last hidden state the model produces from the\n",
        "input sequence.\n",
        "\n",
        "Here is an example of using the tokenizer and model and leveraging the\n",
        "`PreTrainedModel.top_k_top_p_filtering` method to sample the next token following an input sequence\n",
        "of tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8mfXD4L-ZMs"
      },
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer, top_k_top_p_filtering\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"gpt2\")\n",
        "sequence = f\"Hugging Face is based in DUMBO, New York City, and \"\n",
        "input_ids = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "# get logits of last hidden state\n",
        "next_token_logits = model(input_ids).logits[:, -1, :]\n",
        "# filter\n",
        "filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)\n",
        "# sample\n",
        "probs = F.softmax(filtered_next_token_logits, dim=-1)\n",
        "next_token = torch.multinomial(probs, num_samples=1)\n",
        "generated = torch.cat([input_ids, next_token], dim=-1)\n",
        "resulting_string = tokenizer.decode(generated.tolist()[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpULOLx0-ZMs"
      },
      "source": [
        "This outputs a (hopefully) coherent next token following the original sequence, which in our case is the word **has**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHzm0TFJ-ZMs"
      },
      "source": [
        "print(resulting_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQFoV5oM-ZMu"
      },
      "source": [
        "In the next section, we show how this functionality is leveraged in `PreTrainedModel.generate` to\n",
        "generate multiple tokens up to a user-defined length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZRvqHBm-ZMu"
      },
      "source": [
        "### Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ3yetUs-ZMu"
      },
      "source": [
        "In text generation (**a.k.a** **open-ended text generation**) the goal is to create a coherent portion of text that is a\n",
        "continuation from the given context. The following example shows how **GPT-2** can be used in pipelines to generate text.\n",
        "As a default all models apply **Top-K** sampling when used in pipelines, as configured in their respective configurations\n",
        "(see [gpt-2 config](https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json) for example)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emadcB78-ZMv"
      },
      "source": [
        "from transformers import pipeline\n",
        "text_generator = pipeline(\"text-generation\")\n",
        "print(text_generator(\"As far as I am concerned, I will\", max_length=50, do_sample=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxjVQiU3-ZMw"
      },
      "source": [
        "Here, the model generates a random text with a total maximal length of **50** tokens from context **\"As far as I am\n",
        "concerned, I will\"**. The default arguments of `PreTrainedModel.generate()` can be directly overridden in the\n",
        "pipeline, as is shown above for the argument `max_length`.\n",
        "\n",
        "Here is an example of text generation using `XLNet` and its tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHqrRI7R-ZMw"
      },
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").cuda()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set model decoder to true\n",
        "model.config.is_decoder=True\n",
        "# set text-generation params under task_specific_params\n",
        "model.config.task_specific_params[\"text-generation\"] = {\n",
        "    \"do_sample\": True,\n",
        "    \"max_length\": 70,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_k\": 70,\n",
        "    \"no_repeat_ngram_size\": 2\n",
        "} "
      ],
      "metadata": {
        "id": "YbjjPEsIs_Ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now take a look at how a pretrained GPT-2 model completes certain sentences and *why* it completed them as it does:"
      ],
      "metadata": {
        "id": "nIiyMq5uSCFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent = [\"Computational social science is the academic sub-discipline concerned with computational approaches to\"]"
      ],
      "metadata": {
        "id": "OdUNADbstDS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.Explainer(model, tokenizer)\n",
        "shap_values = explainer(sent)"
      ],
      "metadata": {
        "id": "tltpYaLmtDnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.text(shap_values)"
      ],
      "metadata": {
        "id": "vPql-x3ytDuo",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bingo! We see that 'social science.' is indeed an obvious logical end the sentence. The model explanation is also clearly intuitive. But this was a too easy - how about a more difficult query?"
      ],
      "metadata": {
        "id": "7dtrBpSESW3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent = ['Computational social science work increasingly relies on the greater availability of large']"
      ],
      "metadata": {
        "id": "7OKFz3F6tDxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.Explainer(model, tokenizer)\n",
        "shap_values = explainer(sent)"
      ],
      "metadata": {
        "id": "dh1f_1Tsys0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.text(shap_values)"
      ],
      "metadata": {
        "id": "IRdd2vBtys4v",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "More impressive! Perhaps most interesting is the reasoning our model employs: 'scale' follows due to 'large', but 'data sets' are partly inferred from the context of 'science' and 'relies'. The model picks up on the notion that 'Science relies on data'!"
      ],
      "metadata": {
        "id": "jjMfdhkfS9E7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms4-QUv9-ZMx"
      },
      "source": [
        "Text generation is currently possible with **GPT-2**, **OpenAi-GPT**, **CTRL**, **XLNet**, **Transfo-XL** and **Reformer** in\n",
        "PyTorch and for most models in Tensorflow as well. **XLNet** and **Transfo-XL** often\n",
        "need to be padded to work well. GPT-2 is usually a good choice for **open-ended text generation** because it was trained\n",
        "on millions of webpages with a causal language modeling objective.\n",
        "\n",
        "For more information on how to apply different decoding strategies for text generation, please also refer to this text\n",
        "generation blog post [here](https://huggingface.co/blog/how-to-generate)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xyZikOT-ZMx"
      },
      "source": [
        "### Named Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3naHHJ6E-ZMx"
      },
      "source": [
        "Named Entity Recognition (NER) is the task of classifying tokens according to a class, for example, identifying a token\n",
        "as a person, an organisation or a location. An example of a named entity recognition dataset is the CoNLL-2003 dataset,\n",
        "which is entirely based on that task. If you would like to fine-tune a model on an NER task, you may leverage the\n",
        "[run_ner.py](https://github.com/huggingface/transformers/tree/master/examples/token-classification/run_ner.py)\n",
        "(PyTorch), [run_pl_ner.py](https://github.com/huggingface/transformers/tree/master/examples/token-classification/run_pl_ner.py) (leveraging\n",
        "pytorch-lightning) or the [run_tf_ner.py](https://github.com/huggingface/transformers/tree/master/examples/token-classification/run_tf_ner.py) (TensorFlow)\n",
        "scripts.\n",
        "\n",
        "Here is an example of using pipelines to do named entity recognition, specifically, trying to identify tokens as\n",
        "belonging to one of 9 classes:\n",
        "\n",
        "- O, Outside of a named entity\n",
        "- B-MIS, Beginning of a miscellaneous entity right after another miscellaneous entity\n",
        "- I-MIS, Miscellaneous entity\n",
        "- B-PER, Beginning of a person's name right after another person's name\n",
        "- I-PER, Person's name\n",
        "- B-ORG, Beginning of an organisation right after another organisation\n",
        "- I-ORG, Organisation\n",
        "- B-LOC, Beginning of a location right after another location\n",
        "- I-LOC, Location\n",
        "\n",
        "It leverages a fine-tuned model on CoNLL-2003, fine-tuned by [@stefan-it](https://github.com/stefan-it) from [dbmdz](https://github.com/dbmdz)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJciNeGO-ZMy"
      },
      "source": [
        "from transformers import pipeline\n",
        "nlp = pipeline(\"ner\")\n",
        "sequence = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very close to the Manhattan Bridge which is visible from the window.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU0UdGTt-ZMy"
      },
      "source": [
        "This outputs a list of all words that have been identified as one of the entities from the 9 classes defined above.\n",
        "Here are the expected results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZnjywhw-ZMy"
      },
      "source": [
        "print(nlp(sequence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AHQ2Sqn-ZMy"
      },
      "source": [
        "Note how the tokens of the sequence \"Hugging Face\" have been identified as an organisation, and \"New York City\",\n",
        "\"DUMBO\" and \"Manhattan Bridge\" have been identified as locations.\n",
        "\n",
        "Here is an example of doing named entity recognition, using a model and a tokenizer. The process is the following:\n",
        "\n",
        "1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a BERT model and loads it\n",
        "   with the weights stored in the checkpoint.\n",
        "2. Define the label list with which the model was trained on.\n",
        "3. Define a sequence with known entities, such as \"Hugging Face\" as an organisation and \"New York City\" as a location.\n",
        "4. Split words into tokens so that they can be mapped to predictions. We use a small hack by, first, completely\n",
        "   encoding and decoding the sequence, so that we're left with a string that contains the special tokens.\n",
        "5. Encode that sequence into IDs (special tokens are added automatically).\n",
        "6. Retrieve the predictions by passing the input to the model and getting the first output. This results in a\n",
        "   distribution over the 9 possible classes for each token. We take the argmax to retrieve the most likely class for\n",
        "   each token.\n",
        "7. Zip together each token with its prediction and print it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7le9YPL9-ZMz"
      },
      "source": [
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "import torch\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "label_list = [\n",
        "    \"O\",       # Outside of a named entity\n",
        "    \"B-MISC\",  # Beginning of a miscellaneous entity right after another miscellaneous entity\n",
        "    \"I-MISC\",  # Miscellaneous entity\n",
        "    \"B-PER\",   # Beginning of a person's name right after another person's name\n",
        "    \"I-PER\",   # Person's name\n",
        "    \"B-ORG\",   # Beginning of an organisation right after another organisation\n",
        "    \"I-ORG\",   # Organisation\n",
        "    \"B-LOC\",   # Beginning of a location right after another location\n",
        "    \"I-LOC\"    # Location\n",
        "]\n",
        "sequence = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very\" \\\n",
        "           \"close to the Manhattan Bridge.\"\n",
        "# Bit of a hack to get the tokens with the special tokens\n",
        "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
        "inputs = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "outputs = model(inputs).logits\n",
        "predictions = torch.argmax(outputs, dim=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRmyFjWv-ZMz"
      },
      "source": [
        "This outputs a list of each token mapped to its corresponding prediction. Differently from the pipeline, here every\n",
        "token has a prediction as we didn't remove the \"0\"th class, which means that no particular entity was found on that\n",
        "token. The following array should be the output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVGGzja3-ZMz"
      },
      "source": [
        "print([(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].numpy())])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H5fFtGt-ZM0"
      },
      "source": [
        "### Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE9OrGAJ-ZM0"
      },
      "source": [
        "Summarization is the task of summarizing a document or an article into a shorter text.\n",
        "\n",
        "An example of a summarization dataset is the CNN / Daily Mail dataset, which consists of long news articles and was\n",
        "created for the task of summarization. If you would like to fine-tune a model on a summarization task, various\n",
        "approaches are described in this [document](https://github.com/huggingface/transformers/blob/master/examples/seq2seq/README.md).\n",
        "\n",
        "Here is an example of using the pipelines to do summarization. It leverages a Bart model that was fine-tuned on the CNN\n",
        "/ Daily Mail data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBmfYCc6-ZM0"
      },
      "source": [
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
        "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
        "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
        "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
        "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
        "2010 marriage license application, according to court documents.\n",
        "Prosecutors said the marriages were part of an immigration scam.\n",
        "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
        "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
        "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
        "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
        "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
        "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
        "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
        "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
        "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
        "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNppiVo2-ZM0"
      },
      "source": [
        "Because the summarization pipeline depends on the `PreTrainedModel.generate()` method, we can override the default\n",
        "arguments of `PreTrainedModel.generate()` directly in the pipeline for `max_length` and `min_length` as shown\n",
        "below. This outputs the following summary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6GAHEEl-ZM0"
      },
      "source": [
        "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn9S6Rmn-ZM1"
      },
      "source": [
        "Here is an example of doing summarization using a model and a tokenizer. The process is the following:\n",
        "\n",
        "1. Instantiate a tokenizer and a model from the checkpoint name. Summarization is usually done using an encoder-decoder\n",
        "   model, such as `Bart` or `T5`.\n",
        "2. Define the article that should be summarized.\n",
        "3. Add the T5 specific prefix \"summarize: \".\n",
        "4. Use the `PreTrainedModel.generate()` method to generate the summary.\n",
        "\n",
        "In this example we use Google`s T5 model. Even though it was pre-trained only on a multi-task mixed dataset (including\n",
        "CNN / Daily Mail), it yields very good results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rfuam6GI-ZM1"
      },
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "# T5 uses a max_length of 512 so we cut the article to 512 tokens.\n",
        "inputs = tokenizer.encode(\"summarize: \" + ARTICLE, return_tensors=\"pt\", max_length=512)\n",
        "outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(outputs.tolist()[0])"
      ],
      "metadata": {
        "id": "tIbek7rUflxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8gF8TN1-ZM1"
      },
      "source": [
        "### Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBB0eYEv-ZM1"
      },
      "source": [
        "We have already seen Translation examples early in this notebook. We will now use a different case with BERT.\n",
        "\n",
        "An example of a translation dataset is the WMT English to German dataset, which has sentences in English as the input\n",
        "data and the corresponding sentences in German as the target data. If you would like to fine-tune a model on a\n",
        "translation task, various approaches are described in this [document](https://github.com/huggingface/transformers/blob/master/examples/seq2seq/README.md).\n",
        "\n",
        "Here is an example of using the pipelines to do translation. It leverages a T5 model that was only pre-trained on a\n",
        "multi-task mixture dataset (including WMT), yet, yielding impressive translation results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnKOrsbD-ZM1"
      },
      "source": [
        "from transformers import pipeline\n",
        "translator = pipeline(\"translation_en_to_de\")\n",
        "print(translator(\"Hugging Face is a technology company based in New York and Paris\", max_length=40))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esSGSUHh-ZM2"
      },
      "source": [
        "Because the translation pipeline depends on the `PreTrainedModel.generate()` method, we can override the default\n",
        "arguments of `PreTrainedModel.generate()` directly in the pipeline as is shown for `max_length` above.\n",
        "\n",
        "Here is an example of doing translation using a model and a tokenizer. The process is the following:\n",
        "\n",
        "1. Instantiate a tokenizer and a model from the checkpoint name. Summarization is usually done using an encoder-decoder\n",
        "   model, such as `Bart` or `T5`.\n",
        "2. Define the article that should be summarized.\n",
        "3. Add the T5 specific prefix \"translate English to German: \"\n",
        "4. Use the `PreTrainedModel.generate()` method to perform the translation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKIy-6ba-ZM2"
      },
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "model = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "inputs = tokenizer.encode(\"translate English to German: Hugging Face is a technology company based in New York and Paris\", return_tensors=\"pt\")\n",
        "outputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PMky27P-ZM2"
      },
      "source": [
        "As with the pipeline example, we get the same translation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfsUBnVb-ZM2"
      },
      "source": [
        "print(tokenizer.decode(outputs[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47yVwspLAvZx"
      },
      "source": [
        "## Fine tuning models  \n",
        "\n",
        "One of the most exciting things about BERT and GPT is being able to retune them the way we want to. This is what allows us to unleash the full power of BERT, whether it is for classification, generation, or language modelling. \n",
        "\n",
        "The first section contains the CoLA (Linguistic Acceptability) classification task, the second contains training a model on Trump tweets, and the last bit has us training a model on US and UK blog posts. In each of these cases we are adapting our model to a specific domain.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nFkME77f1Zh"
      },
      "source": [
        "### Fine tuning sequence classification\n",
        "\n",
        "To demonstrate this, we will use the [Corpus of Linguistic Acceptability](https://nyu-mll.github.io/CoLA/). This corpus has sentences which are labelled as \"acceptable\" and \"unnacceptable\", and is a test of natural language understanding.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwp7W3BWWERu"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers import AdamW, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPbf8Yvyf-Hn"
      },
      "source": [
        "We'll now load the CoLA dataset up here to Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZH0HWkNgbho"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRqSO7DUWXmR"
      },
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading dataset...')\n",
        "\n",
        "# The URL for the dataset zip file.\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "# Download the file (if we haven't already)\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dewGhdzgEkH"
      },
      "source": [
        "# Unzip the dataset (if we haven't already)\n",
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzyIo3XfgVxJ"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onaOGdjxgiyG"
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saNk183jgjVP"
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = df.label.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOLRljp3gr9T"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wywdbBISguFj"
      },
      "source": [
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfXhgejEgxRY"
      },
      "source": [
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko6fHIlAiHm7"
      },
      "source": [
        "import os\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSaU7DbkgzWh"
      },
      "source": [
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEUBxUm8g0ov"
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4QPyBRDiIx1"
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2020, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2020, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLwGeP3u6s0C"
      },
      "source": [
        "#### How do shallow networks do?\n",
        "\n",
        "Just to get a reference of how easy/difficult this task is, let's train a keras LSTM model to perform this classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWLfSUPjiLJA"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ0A0HbAiMpc"
      },
      "source": [
        "vocab_in_size = tokenizer.vocab_size\n",
        "embedding_dim = 32\n",
        "unit = 100\n",
        "no_labels = len(np.unique(train_labels))\n",
        "batch_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POlslC2AiPAI"
      },
      "source": [
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
        "model_lstm.add(LSTM(unit))\n",
        "model_lstm.add(Dense(no_labels, activation='softmax'))\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu2c6CWSiP3D"
      },
      "source": [
        "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
        "                              epochs=10,batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6muAlDeiac7"
      },
      "source": [
        "#### On with BERT!\n",
        "\n",
        "So while Neural Networks can do a good job with some kind of classification tasks, they don't perform too well on intent classification. Let us see how BERT might do. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJDljt39iddQ"
      },
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgmJ-E_VigTt"
      },
      "source": [
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX6t1SIUi22s"
      },
      "source": [
        "#### Loading our pre-trained model\n",
        "\n",
        "#### Train Model\n",
        "Now that our input data is properly formatted, it's time to fine tune the BERT model.\n",
        "For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task. (Note that sometimes fine-tuning is also called pre-training the model.) Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built atop a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n",
        "We'll load BertForSequenceClassification. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
        "\n",
        "#### Structure of Fine-Tuning Model\n",
        "\n",
        "As we've showed before, the first token of every sequence is the special classification token ([CLS]). Unlike the hidden state vector corresponding to a normal word token, the hidden state corresponding to this special token is designated by the authors of BERT as an aggregate representation of the whole sentence used for classification tasks. As such, when we feed in an input sentence to our model during training, the output is the length 768 hidden state vector corresponding to this token. The additional layer that we've added on top consists of untrained linear neurons of size [hidden_state, number_of_labels], so [768,2], meaning that the output of BERT plus our classification layer is a vector of two numbers representing the \"score\" for \"grammatical/non-grammatical\" that are then fed into a cross-entropy loss function.\n",
        "\n",
        "#### The Fine-Tuning (or, confusingly, Pre-Training) Process\n",
        "\n",
        "Because the pre-trained BERT layers already encode a lot of information about the language, training the classifier is relatively inexpensive. Rather than training every layer in a large model from scratch, it's as if we have already trained the bottom layers 95% of where they need to be, and only really need to train the top layer, with a bit of tweaking going on in the lower levels to accomodate our task.\n",
        "Sometimes practicioners will opt to \"freeze\" certain layers when fine-tuning, or to apply different learning rates, apply diminishing learning rates, etc. all in an effort to preserve the good quality weights in the network and speed up training (often considerably). In fact, recent research on BERT specifically has demonstrated that freezing the majority of the weights results in only minimal accuracy declines, but there are exceptions and broader rules of transfer learning that should also be considered. For example, if your task and fine-tuning dataset is very different from the dataset used to train the transfer learning model, freezing the weights may not be a good idea. We'll cover the broader scope of transfer learning in NLP in a future post.\n",
        "OK, let's load BERT! There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYkR2tJxiiJk"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfwPyoh9ii0x"
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "likqi1-FimRZ"
      },
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJcXKErgioEE"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-fvXJW_ip1k"
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtCJt0A0irEL"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnwi-bj1itbL"
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZPeyGKYtoCn"
      },
      "source": [
        "loss_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFogz7jeiycc"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3S6GaadjK48"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_fqVB6CtzTo"
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIoseqqzt1pc"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "\n",
        "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "    # in to a list of 0s and 1s.\n",
        "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "    # Calculate and store the coef for this batch.  \n",
        "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "    matthews_set.append(matthews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BreKUJ9t25l"
      },
      "source": [
        "matthews_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20s0Poipt6XT"
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tqQ5tX2C6K_"
      },
      "source": [
        "from sklearn.metrics import classification_report\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLFKqDerDw3n"
      },
      "source": [
        "print(classification_report(flat_true_labels, flat_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpLhjm3D9-vA"
      },
      "source": [
        "We see that BERT performs a lot better  on the Linguistic Acceptability task!\n",
        "\n",
        "We now want to save this model to disk. Once you save it to disk, right click on the file and save it to your local computer to use it on your notebook. You would have trained your model on your own dataset, in which case you would also need to upload your data, or load it using wget as we did before.\n",
        "\n",
        "NOTE: the files are accessible, both uploading and downloading, on the left hand side of the page, under the \"folder\" section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ1Y-b7-90S9"
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq7G7JbAt9sn"
      },
      "source": [
        "BERT is a powerful tool for sequence classification and we encourage you to try it out for your dataset of choice!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCyfnZ7i-NzR"
      },
      "source": [
        "### Fine-tuning text generation\n",
        "\n",
        "You have to upload your files to the colab file - on the left of the screen, on the files section, click the upload section, and upload test_text_trump, train_text_trump, run_generation.py, and run_language_modelling.py. These files would be on this [GitHub repository page](https://github.com/UChicago-Computational-Content-Analysis/Homework-Notebooks/tree/main/week-8).\n",
        "\n",
        "We start with training a model on Trump tweets. The following two lines of code does the language training, which we will then use for text generation. The important part here is training your model. You will notice it being saved in the files section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XdVsM_cL_3O"
      },
      "source": [
        "torch.cuda.empty_cache() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqiVuxFuLLvu"
      },
      "source": [
        "!python /content/run_language_modelling.py --output_dir=output_gpt_trump --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=/content/train_text_trump --do_eval --eval_data_file=/content/test_text_trump --per_gpu_train_batch_size=1 --per_gpu_eval_batch_size=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1LRPvhkP8Mm"
      },
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu-a109sOcS5"
      },
      "source": [
        "tokenizer_trump = AutoTokenizer.from_pretrained(\"/content/output_gpt_trump\")\n",
        "model_trump = AutoModelWithLMHead.from_pretrained(\"/content/output_gpt_trump\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXi4jQIdwkHN"
      },
      "source": [
        "Now that we've loaded our fine-tuned Trump GPT model, let's see what the model thinks of Obama."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ov5YOjtOh0C"
      },
      "source": [
        "sequence = \"Obama is going to\"\n",
        "\n",
        "input = tokenizer_trump.encode(sequence, return_tensors=\"pt\")\n",
        "generated = model_trump.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
        "\n",
        "resulting_string = tokenizer_trump.decode(generated.tolist()[0])\n",
        "print(resulting_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEFIJaHnxcE2"
      },
      "source": [
        "Yikes. Sounds like what we'd expect from a Trump bot. Let's now try with the standard, untuned GPT2 model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ye9REQXyd3R"
      },
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "\n",
        "tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model_gpt = AutoModelWithLMHead.from_pretrained(\"gpt2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf7GRWQ8Okmu"
      },
      "source": [
        "sequence = \"Obama is going to\"\n",
        "\n",
        "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
        "generated = model_gpt.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
        "\n",
        "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
        "print(resulting_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-CDbhF8SQoN"
      },
      "source": [
        "We see dramatically different results for the two models!\n",
        "\n",
        "### Fine tuning language models\n",
        "\n",
        "We will also look at an example of training BERT-like models, such as RoBERTa, on US and UK blog posts. Our hope is to introduce an \"accent\" in a model, which learns distrinctive aspects of the kinds of speech - but considering how similar they can be and that we don't have access to a _lot_ of data, so it may not be perfect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwsrsQ2qSVnN"
      },
      "source": [
        "!python run_language_modelling.py --output_dir=output_roberta_US --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=us_blog_train --do_eval --eval_data_file=us_blog_test --mlm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R80-_g-xpiwx"
      },
      "source": [
        "!python run_language_modelling.py --output_dir=output_roberta_GB --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=gb_blog_train --do_eval --eval_data_file=gb_blog_test --mlm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC_wG9PGPIAx"
      },
      "source": [
        "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N1_R-9EPIoY"
      },
      "source": [
        "roberta_us_model_embedding = RobertaModel.from_pretrained('/content/roberta_us')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmWY040oPKw-"
      },
      "source": [
        "roberta_us_tokenizer = RobertaTokenizer.from_pretrained('/content/roberta_us')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrJ0SUEgPPYR"
      },
      "source": [
        "Let us try to visualise how words in a sentence or different or similar to each other. We will try to construct sentences where words might mean different things in different countries - in the US, people might eat chips with salsa, but in the UK, chips are what Americans call french fries, and might eat it fried fish instead. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YAHxlZyPKqI"
      },
      "source": [
        "text = \"Do you have your chips with fish or with salsa?\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lAOBNvYPSjh"
      },
      "source": [
        "text1 = \"He went out in just his undershirt and pants.\" #pants are underwear in Britain; maybe closer to an undershirt\n",
        "text2 = \"His braces completed the outfit.\" #braces are suspenders (in Britain); maybe closer to an outfit\n",
        "text3 = \"Does your pencil have a rubber on it?\" #rubber is an eraser in Britain); maybe closer to a pencil\n",
        "text4 = \"Was the bog closer to the forest or the house?\" #bog is a toilen in Britain); maybe closer to a house\n",
        "text5 = \"Are you taking the trolley or the train to the grocery market\" #trolley is a food carriage; possibly closer to a market"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4GJGzrsPSXm"
      },
      "source": [
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlPmWCSPPX9A"
      },
      "source": [
        "def visualise_diffs(text, model, tokenizer):\n",
        "    word_vecs = []\n",
        "    for i in range(0, len(text.split())):\n",
        "        word_vecs.append(word_vector(text, i, model, tokenizer))\n",
        "    L = []\n",
        "    for p in word_vecs:\n",
        "        l = []\n",
        "        for q in word_vecs:\n",
        "            l.append(1 - cosine(p, q))\n",
        "        L.append(l)\n",
        "    M = np.array(L)\n",
        "    fig = plt.figure()\n",
        "    div = pd.DataFrame(M, columns = list(text.split()), index = list(text.split()))\n",
        "    ax = sns.heatmap(div)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMe1SrUOPbZs"
      },
      "source": [
        "visualise_diffs(text, roberta_us_model_embedding, roberta_us_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pksnpt6hPdU4"
      },
      "source": [
        "roberta_gb_model_embedding = RobertaModel.from_pretrained('/content/roberta_gb')\n",
        "roberta_gb_tokenizer = RobertaTokenizer.from_pretrained('/content/roberta_gb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kus6hDHSPfCG"
      },
      "source": [
        "visualise_diffs(text, roberta_gb_model_embedding, roberta_gb_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOcuLpiMPjGv"
      },
      "source": [
        "What do you see regarding the relations with chips and sala/fish? What about the other sentences? How about comparing sentence embeddings?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46JPmKEzBcJ1"
      },
      "source": [
        "### Post-training (Pre-training) and Domain Specific Models\n",
        "\n",
        "After you finish training your models, you can download them by right clicking on each of the 8 files inside the name of your output folder. The largest file, pytorch_model.bin, is the file which does most of the heavy lifting but you need the other files to be able to use it later.\n",
        "\n",
        "We also recommend checking out other domain specific models, such as [sciBERT](https://github.com/allenai/scibert) and [bioBERT](https://github.com/dmis-lab/biobert). This link should be useful for this [domain BERT tutorial](https://mccormickml.com/2020/06/22/domain-specific-bert-tutorial/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0GYy34n1zmB"
      },
      "source": [
        "# BERT Word and Sentence Embeddings\n",
        "\n",
        "In this section we will explore the many ways we can use BERT to generate word and sentence embeddings.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIKFzr2s7ZtK"
      },
      "source": [
        "### Embeddings, Context Words\n",
        "\n",
        "We saw how a bootstrapped BERT model performed so much better than a model trained from scatch. Because BERT's method of capturing context is bidirectional, meaning that words can now have different word embedding values based on their location within a sentence. Let us use the same BERT model to capture sentence and word embeddings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "sFlXkaFf7ZtK"
      },
      "source": [
        "from transformers import BertModel\n",
        "from transformers import BertTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crmHvLsp7ZtK"
      },
      "source": [
        "Let's go through the sentence format for the BERT model, as well as how our vocabulary looks like. Note that you have to use the BERT tokenizer to use the BERT model because of the similar vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "o4goTAyK7ZtK"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnCAUot67ZtK"
      },
      "source": [
        "text = \"Here is the sentence I want embeddings for.\"\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Tokenize our sentence with the BERT tokenizer.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Print out the tokens.\n",
        "print (tokenized_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzcdpcIK7ZtL"
      },
      "source": [
        "The BERT model uses a WordPiece technique to do its tokenizing, as described in the initial published paper. That's why the word embedding is split up the way it is.\n",
        "A quick peek at what the voabulary looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hVi4xx57ZtL"
      },
      "source": [
        "list(tokenizer.vocab.keys())[6000:6030]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHBVq2rk7ZtL"
      },
      "source": [
        "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
        "text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
        "\n",
        "# Add the special tokens.\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Split the sentence into tokens.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Map the token strings to their vocabulary indices.\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "# Display the words with their indices.\n",
        "for tup in zip(tokenized_text, indexed_tokens):\n",
        "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uURNyYKD7ZtL"
      },
      "source": [
        "#### Segment ID\n",
        "\n",
        "BERT is trained on and expects sentence pairs, using 1s and 0s to distinguish between the two sentences. That is, for each token in “tokenized_text,” we must specify which sentence it belongs to: sentence 0 (a series of 0s) or sentence 1 (a series of 1s). For our purposes, single-sentence inputs only require a series of 1s, so we will create a vector of 1s for each token in our input sentence.\n",
        "\n",
        "If you want to process two sentences, assign each word in the first sentence plus the ‘[SEP]’ token at 0, and all tokens of the second sentence a 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWF4CpzR7ZtM"
      },
      "source": [
        "segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "print (segments_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oY0Goj27ZtN"
      },
      "source": [
        "As we did for classification, we now convert these segments to tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8Bt1vwX7ZtN"
      },
      "source": [
        "The embedding layer is the hidden state layer, and this is what we pick up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "HkgVruQM7ZtN"
      },
      "source": [
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkYDOGeg7ZtO"
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model_embedding = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model_embedding.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "1uEUCKHD7ZtO"
      },
      "source": [
        "output = model_embedding(tokens_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_tensor.shape"
      ],
      "metadata": {
        "id": "jS1Xglbq0GZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "S5rDTvJR7ZtO"
      },
      "source": [
        "output[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uwa_CRL47ZtO"
      },
      "source": [
        "output[1].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_XYjM4p7ZtP"
      },
      "source": [
        "#### Understanding the Output\n",
        "\n",
        "This kind of forward pass returns us the last layer of the net, which we will use to make our vectors. The first object returned contains the batch number, followed by each of the tokens and their vector values. The second object contains a vector value, which I suspect is the sentence vector of the tokens. \n",
        "\n",
        "The first index is the batch size, and our batch size is 1, so we just choose the 0th index and work with that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "tXlaU_EO7ZtP"
      },
      "source": [
        "word_embeddings, sentence_embedding = output[0], output[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr1Fe4IZ7ZtP"
      },
      "source": [
        "len(word_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m91OJMw17ZtP"
      },
      "source": [
        "word_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J25Lkeb7ZtQ"
      },
      "source": [
        "Let’s take a quick look at the range of values for a given layer and token.\n",
        "\n",
        "You’ll find that the range is fairly similar for all layers and tokens, with the majority of values falling between [-2, 2], and a small smattering of values around -10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROd5AAVzP951"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D6zo2IJ7ZtQ"
      },
      "source": [
        "vec = word_embeddings[0][0]\n",
        "vec = vec.detach().numpy()\n",
        "# Plot the values as a histogram to show their distribution.\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.hist(vec, bins=200)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t16xoZYq7ZtQ"
      },
      "source": [
        "These values are grouped by layer - we can use the permute function to make it grouped by each individual token instead. Let us look at what the latter looks like:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xd8NOQB7ZtQ"
      },
      "source": [
        "#### Word Vectors\n",
        "\n",
        "So each of those tokens have embedding values - let us try and compare them with each other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "aJO51oGQ7ZtR"
      },
      "source": [
        "token_vecs = []\n",
        "# For each token in the sentence...\n",
        "for embedding in word_embeddings[0]:\n",
        "    cat_vec = embedding.detach().numpy()\n",
        "    # Use `cat_vec` to represent `token`.\n",
        "    token_vecs.append(cat_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oZR3FKN7ZtR"
      },
      "source": [
        "len(token_vecs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5KWAimA7ZtR"
      },
      "source": [
        "Another method to create the vectors is to sum the last four layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhCeAYWN7ZtR"
      },
      "source": [
        "#### Sentence Vector\n",
        "\n",
        "To get a single vector for our entire sentence we have multiple application-dependent strategies - we could just average all the tokens in our sentence. We can also use this oppurtunity to see if the second vector returned is a sentence vector too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "UlahGmRm7ZtS"
      },
      "source": [
        "sentence_embedding_0 = sentence_embedding.detach().numpy()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Nyv_QXsk7ZtS"
      },
      "source": [
        "sentence_embedding_1 = np.mean(token_vecs, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikFTzpVp7ZtS"
      },
      "source": [
        "len(sentence_embedding_0), len(sentence_embedding_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAB1xXXf7ZtT"
      },
      "source": [
        "Remember that the power of these vectors is how they are context dependant - our sentence had multiple uses of the word bank. Let us see the index and the word of the sentence and check the context accordingly. We'll then print the simlarity values for the similar and different meanings and see how it turns out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr0_tT2g7ZtT"
      },
      "source": [
        "for i, token_str in enumerate(tokenized_text):\n",
        "    print(i, token_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SLTbo087ZtT"
      },
      "source": [
        "print('First 5 vector values for each instance of \"bank\".')\n",
        "print('')\n",
        "print(\"bank vault   \", str(token_vecs[6][:5]))\n",
        "print(\"bank robber  \", str(token_vecs[10][:5]))\n",
        "print(\"river bank   \", str(token_vecs[19][:5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3DMy8cah0B-"
      },
      "source": [
        "This means that within the space inscribed by BERT vectors, each word is represented not by a point (or vector) in the space, but rather by a cloud of points (or vectors)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "um9VIP-D7ZtT"
      },
      "source": [
        "from scipy.spatial.distance import cosine"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYJAau6T7ZtT"
      },
      "source": [
        "\n",
        "# Calculate the cosine similarity between the word bank \n",
        "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
        "diff_bank = 1 - cosine(token_vecs[10], token_vecs[19])\n",
        "\n",
        "# Calculate the cosine similarity between the word bank\n",
        "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
        "same_bank = 1 - cosine(token_vecs[10], token_vecs[6])\n",
        "\n",
        "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
        "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BesrJbg7ZtU"
      },
      "source": [
        "This makes sense! Let us see if the mean value of all the tokens and what we think is the sentence vector is the same thing, by checking their cosine distance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPRi2ErU7ZtU"
      },
      "source": [
        "1 - cosine(sentence_embedding_0, sentence_embedding_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7xbAiqb7ZtU"
      },
      "source": [
        "That is good - it seems it is indeed the sentence vector, so we can now write two functions which calculate the word and sentence vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ZioE7gS77ZtU"
      },
      "source": [
        "def word_vector(text, word_id, model, tokenizer):\n",
        "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    output = model(tokens_tensor)\n",
        "    word_embeddings, sentence_embeddings = output[0], output[1]\n",
        "    vector = word_embeddings[0][word_id].detach().numpy()\n",
        "    return vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "wsl46pjQ7ZtU"
      },
      "source": [
        "word_10 = word_vector(text, 6, model_embedding, tokenizer)\n",
        "word_6 = word_vector(text, 10, model_embedding, tokenizer)\n",
        "word_19 = word_vector(text, 19, model_embedding, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "YYIlUpJt7ZtV"
      },
      "source": [
        "def sentence_vector(text, model, tokenizer, method=\"average\"):\n",
        "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    output = model(tokens_tensor)\n",
        "    word_embeddings, sentence_embeddings = output[0], output[1]\n",
        "    token_vecs = []\n",
        "    \n",
        "    for embedding in word_embeddings[0]:\n",
        "        cat_vec = embedding.detach().numpy()\n",
        "        token_vecs.append(cat_vec)\n",
        "        \n",
        "    if method == \"average\":\n",
        "        sentence_embedding = np.mean(token_vecs, axis=0)\n",
        "    if method == \"model\":\n",
        "        sentence_embedding = sentence_embeddings\n",
        "    # do something\n",
        "    return sentence_embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "lqAidK2r7ZtV"
      },
      "source": [
        "sen_vec_0 = sentence_vector(text, model_embedding, tokenizer)\n",
        "sen_vec_1 = sentence_vector(text, model_embedding, tokenizer, method=\"model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GsjHA-o7ZtV"
      },
      "source": [
        "#### Similarity metrics\n",
        "It is worth noting that word-level similarity comparisons are not appropriate with BERT embeddings because these embeddings are contextually dependent, meaning that the word vector changes depending on the sentence in which it appears. This enables direct sensitivity to polysemy so that, e.g., your representation encodes river “bank” and not a financial institution “bank”. Nevertheless, it makes direct word-to-word similarity comparisons less valuable. For sentence embeddings, however, similarity comparison is still valid such that one can query, for example, a single sentence against a dataset of other sentences in order to find the most similar. Depending on the similarity metric used, the resulting similarity values will be less informative than the relative ranking of similarity outputs as some similarity metrics make assumptions about the vector space (equally-weighted dimensions, for example) that do not hold for our 768-dimensional vector space.\n",
        "\n",
        "### Using the Vectors\n",
        "\n",
        "Without fine-tuning, BERT features may be less useful than plain GloVe or word2vec.\n",
        "They start to be interesting when you fine-tune a classifier on top of BERT. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcBeb-E0GApS"
      },
      "source": [
        "### BERT Sentence Perplexity\n",
        "\n",
        "BERT can be used as a language model to see how \"likely\" a sentence is in that language model's configurations. \n",
        "\n",
        "Here is some additional reading on the topic:\n",
        "\n",
        "- [Can We Use BERT as a Language Model to Assign a Score to a Sentence?](https://www.scribendi.ai/can-we-use-bert-as-a-language-model-to-assign-score-of-a-sentence/)\n",
        "\n",
        "- [Comparing BERT and GPT-2 as Language Models to Score the Grammatical Correctness of a Sentence](https://www.scribendi.ai/comparing-bert-and-gpt-2-as-language-models-to-score-the-grammatical-correctness-of-a-sentence/)\n",
        "\n",
        "- [Transformers - Perplexity of fixed-length models](https://huggingface.co/transformers/perplexity.html)\n",
        "\n",
        "- [Understanding evaluation metrics for language models](https://thegradient.pub/understanding-evaluation-metrics-for-language-models/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYOx7a1VKE6r"
      },
      "source": [
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkbud6iEJJma"
      },
      "source": [
        "from transformers import BertTokenizer,BertForMaskedLM\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKxudeIGJNML"
      },
      "source": [
        "bertMaskedLM = BertForMaskedLM.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR4c5pp3JSve"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgGODxHsHDLE"
      },
      "source": [
        "def get_score(sentence, model, tokenizer):\n",
        "    tokenize_input = tokenizer.tokenize(sentence)\n",
        "    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
        "    predictions = model(tensor_input)\n",
        "    # print(predictions[0])\n",
        "    # sentence_embedding, word_embedding = predictions[0], predictions[1]\n",
        "    loss_fct = torch.nn.CrossEntropyLoss()\n",
        "    loss = loss_fct(predictions[0].squeeze(),tensor_input.squeeze()).data \n",
        "    return math.exp(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaCzvXPhJbH2"
      },
      "source": [
        "get_score(\"Can you get some bread?\", bertMaskedLM, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvYzCBbFK7X7"
      },
      "source": [
        "get_score(\"This is a correct sentence.\", bertMaskedLM, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82QXPQBDK-2u"
      },
      "source": [
        "get_score(\"Fungus skatebutter\", bertMaskedLM, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmbMW-eHKHTn"
      },
      "source": [
        "get_score(\"Xuusgs ddd\", bertMaskedLM, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2DwGvS9LEKc"
      },
      "source": [
        "A lower perplexity score suggests a sentence with a higher probability of being correct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zpo1ZO-12KB"
      },
      "source": [
        "### Contextual Sentence Embeddings\n",
        "\n",
        "Standard BERT sentence embeddings don't perform as well as special architectures built for sentence embeddings, such as BERT siamese networks. \n",
        "\n",
        "We will use the [UKPLab package \"sentence transformers\"](https://github.com/UKPLab/sentence-transformers\n",
        "), which implement such methods.\n",
        "\n",
        "- Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (EMNLP 2019)\n",
        "- Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation (EMNLP 2020)\n",
        "- Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks (NAACL 2021)\n",
        "- The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes (arXiv 2020)\n",
        "\n",
        "Documentation for [SBERT](https://www.sbert.net/examples/applications/computing-embeddings/README.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTm6r2TRHw1C"
      },
      "source": [
        "!pip install sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K432TawSH0mV"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtnxsyhoH1nk"
      },
      "source": [
        "\n",
        "#Our sentences we like to encode\n",
        "sentences = ['This framework generates embeddings for each input sentence',\n",
        "    'Sentences are passed as a list of string.', \n",
        "    'The quick brown fox jumps over the lazy dog.']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jspJw3pH24w"
      },
      "source": [
        "#Sentences are encoded by calling model.encode()\n",
        "embeddings = model.encode(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDkn9wcaZ1mD"
      },
      "source": [
        "#Print the embeddings\n",
        "for sentence, embedding in zip(sentences, embeddings):\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Embedding:\", embedding)\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-ha_CxEIB45"
      },
      "source": [
        "We recommend visiting the documentation to view other uses of these sentence embeddings, such as [clustering](https://www.sbert.net/examples/applications/clustering/README.html), [Paraphrase Mining](https://www.sbert.net/examples/applications/paraphrase-mining/README.html), [Semantic Search](https://www.sbert.net/examples/applications/semantic-search/README.html), [Retreive and Rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html), and even multi-modal [Image Search](https://www.sbert.net/examples/applications/image-search/README.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM-2pRuUa5Os"
      },
      "source": [
        "# BERTology\n",
        "\n",
        "\"BERTology\" refers to the growing number of papers disecting BERT and it's inner workings. \n",
        "\n",
        "- [A Primer in BERTology: What we know about how BERT works](https://arxiv.org/abs/2002.12327)\n",
        "\n",
        "- [Huggingface Transformers page on BERTology](https://huggingface.co/transformers/bertology.html)\n",
        "\n",
        "### Visualizing Attention\n",
        "\n",
        "Visualising the Attention layers are a popular way to understand BERT.\n",
        "\n",
        "**We highly recommend this visual blog on [explaining Transformers](http://jalammar.github.io/explaining-transformers/\n",
        ") and this visual blog on [visualising hidden states](http://jalammar.github.io/hidden-states/\n",
        ") before proceeding**\n",
        "\n",
        "The package \"bertviz\" [(GitHub link)](https://github.com/jessevig/bertviz) allows us to look into specific layers of multiple Transformers based models. \n",
        "\n",
        "This [blog post](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1) walks through the architecture of BERT and what exactly we can visualise. If you intend on using the BERT visualisations we recommend going through the blog post.\n",
        "\n",
        "We also point these two other visualisations tools for such models:\n",
        "- [Seq2Seq-Vis](http://seq2seq-vis.io/), [GitHub link](https://github.com/HendrikStrobelt/Seq2Seq-Vis)\n",
        "- [exBERT](https://exbert.net/), [GitHub link](https://github.com/bhoov/exbert)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exm70noGa81x"
      },
      "source": [
        "# code which visualises BERT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-26bESCtirt"
      },
      "source": [
        "!pip install bertviz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48CAczKlt0Ej"
      },
      "source": [
        "# Load model and retrieve attention weights\n",
        "\n",
        "from bertviz import head_view, model_view\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "model_version = 'bert-base-uncased'\n",
        "do_lower_case = True\n",
        "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)\n",
        "sentence_a = \"The cat sat on the mat\"\n",
        "sentence_b = \"The cat lay on the rug\"\n",
        "inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=True)\n",
        "input_ids = inputs['input_ids']\n",
        "token_type_ids = inputs['token_type_ids']\n",
        "attention = model(input_ids, token_type_ids=token_type_ids)[-1]\n",
        "sentence_b_start = token_type_ids[0].tolist().index(1)\n",
        "input_id_list = input_ids[0].tolist() # Batch index 0\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_id_list) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rffjP27Jt26g"
      },
      "source": [
        "## Model View\n",
        "The model view gives a birds-eye view of attention across all of the layers (rows) and heads (columns) in the model. In this case we are showing *bert-base*, which has 12 layers and 12 heads (zero-indexed). \n",
        "\n",
        "### Usage\n",
        "* **Click** on any **cell** for a detailed view of attention for the associated attention head.\n",
        "* Then **hover** over any **token** on the left side of detail view to filter the attention from that token.\n",
        "* The lines show the attention from each token (left) to every other token (right). Darker lines indicate higher attention weights.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT4T5bdEt54v"
      },
      "source": [
        "model_view(attention, tokens, sentence_b_start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYs0L8Ftt_Hu"
      },
      "source": [
        "## Head View\n",
        "The attention-head view visualizes attention in one or more heads in a particular layer in the model.\n",
        "\n",
        "### Usage\n",
        "* **Hover** over any **token** on the left/right side of the visualization to filter attention from/to that token. The colors correspond to different attention heads.\n",
        "* **Double-click** on any of the **colored tiles** at the top to filter to the corresponding attention head.\n",
        "* **Single-click** on any of the **colored tiles** to toggle selection of the corresponding attention head. \n",
        "* **Click** on the **Layer** drop-down to change the model layer (zero-indexed).\n",
        "* The lines show the attention from each token (left) to every other token (right). Darker lines indicate higher attention weights. When multiple heads are selected, the attention weights are overlaid on one another. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eqw4PQnuAcX"
      },
      "source": [
        "head_view(attention, tokens, sentence_b_start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyvp97I4uEj-"
      },
      "source": [
        "## Neuron View\n",
        "The attention-head view visualizes attention, as well as query and key values, in a particuler attention head.\n",
        "\n",
        "**NOTE: This visualization requires Chrome when run in Colab**\n",
        "\n",
        "### Usage\n",
        "* **Hover** over any of the tokens on the left side of the visualization to filter attention from that token.\n",
        "* Then **click** on the **plus** icon that is revealed when hovering. This shows the query vectors, key vectors, and intermediate computations for the attention weights (<span style='color:blue'>blue</span>=positive, <span style='color:#ff6318'>orange</span>=negative).  \n",
        "* Once in the expanded view, **hover** over any other **token** on the left to see the associated attention computations.\n",
        "* **Click** on the **Layer** or **Head** drop-downs to change the model layer or head (zero-indexed)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfNTtsiquD8K"
      },
      "source": [
        "from bertviz.transformers_neuron_view import BertModel, BertTokenizer\n",
        "from bertviz.neuron_view import show\n",
        "\n",
        "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)\n",
        "model_type = 'bert'\n",
        "show(model, model_type, tokenizer, sentence_a, sentence_b, layer=2, head=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q35FQTJ6BEE8"
      },
      "source": [
        "These visualisations essentially allow us to see what attention values are assigned between a pair of sentences within the BERT model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsnU6W6p4e0J"
      },
      "source": [
        "\n",
        "# Homework\n",
        "\n",
        "In this notebook we saw some of the state of the art language models, as well as sequence to sequence models which served as very early versions of such models. Text and language models had its own ImageNet moment with the deluge of large pre-trained language models in 2018 and onwards.\n",
        "\n",
        "For the homework, you will be using BERT, GPT, or a similar large Transformer based model to analyse your social scientific textual dataset.\n",
        "\n",
        "There are a total of 8 standard questions, you will be graded for 4/8, and there are two bonus questions.\n",
        "\n",
        "**1a)** Use a large pre-trained language model for sequence classication, question answering, language modelling, or any other NLP task. Fine-tune the model on your dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggij-U914fvz"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "wiki = pd.read_csv('/content/drive/MyDrive/deep-learning/wiki_movie_plots_deduped.csv')\n",
        "wiki = wiki[~wiki['Genre'].isin(['Unknown', 'unknown'])]\n",
        "\n",
        "wiki = wiki[wiki['Genre'].isin(['horror', 'thriller', 'comedy'])]\n",
        "wiki['len'] = wiki['Plot'].apply(lambda x: len(x.split()))\n",
        "wiki = wiki[wiki['len'] < 500]\n",
        "wiki = pd.concat([df[1].sample(n=min(df[1].shape[0], 1000), random_state=1) for df in wiki.groupby('Genre')], ignore_index=True)\n",
        "\n",
        "wiki['label'] = wiki['Genre'].map({\n",
        "    'comedy': 0, \n",
        "    'horror': 1,\n",
        "    'thriller': 1\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sentence and label lists\n",
        "sentences = wiki['Plot'].values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = wiki['label'].values"
      ],
      "metadata": {
        "id": "g5F8sHbSBNz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, truncation=True, model_max_length=512)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent)[:255] + ['[SEP]'] for sent in sentences]\n",
        "print(\"Tokenize the first plot:\")\n",
        "print(tokenized_texts[1])"
      ],
      "metadata": {
        "id": "v355pJ7MBZ8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 256\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
      ],
      "metadata": {
        "id": "2OHkQrn7BnLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "P6nwfFK9Bvai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "metadata": {
        "id": "4f36iha6GR6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)"
      ],
      "metadata": {
        "id": "d9yhHHV9GVQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2020, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2020, test_size=0.1)"
      ],
      "metadata": {
        "id": "OB6s7fBwGYnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "vocab_in_size = tokenizer.vocab_size\n",
        "embedding_dim = 32\n",
        "unit = 100\n",
        "no_labels = len(np.unique(train_labels))\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "f8a1BuxFG2U-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "metadata": {
        "id": "Z3TURWsIHDsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers import AdamW, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import io\n",
        "\n",
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "uxOosuZbHEqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).cuda()"
      ],
      "metadata": {
        "id": "SxoQMWhGHfRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n"
      ],
      "metadata": {
        "id": "J9CBxkOXHgoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "metadata": {
        "id": "GDS1lwG-HmRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "metadata": {
        "id": "i0wchFiSHxGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "ja-utKKBIFQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "yKyEkpTIH3r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2rdyH-GLyj2"
      },
      "source": [
        "**1b)** What model did you use? How did your model perform?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5Yn_re2L300"
      },
      "source": [
        "model_performance = \"I used BERT and it performed well in genre prediction task using movie plots. Movies in the subsampled dataset belonged to either 'comedy' or 'thriller/horror' genre. The model was able to predict the correct genre 89% of the time on a balanced validation set.\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1c)** Build a SHAP-based explainer of your model for 3 inputs from your data. Describe the feature importance relationships you see for each of your inputs."
      ],
      "metadata": {
        "id": "ij1ktlrnYipD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "pred = transformers.pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0, return_all_scores=True)"
      ],
      "metadata": {
        "id": "DRv4MlepOuJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eg_comedy = wiki.sample(n=1, random_state=1)['Plot'].iat[0]\n",
        "print(eg_comedy)"
      ],
      "metadata": {
        "id": "-y71WcbpO-2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicts LABEL_0 which is the comedy genre\n",
        "pred(eg_comedy)"
      ],
      "metadata": {
        "id": "IL-roLo9O8oU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eg_horror = wiki[wiki['label']==1].sample(n=1, random_state=2)['Plot'].iat[0]\n",
        "print(eg_horror)"
      ],
      "metadata": {
        "id": "qbdznfn5Po2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicts LABEL_1 which is the thriller/horror genre\n",
        "pred(eg_horror)"
      ],
      "metadata": {
        "id": "QXFBre2FP5_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "#Creaing an explainer that wraps around our pipeline\n",
        "explainer = shap.Explainer(pred)\n",
        "\n",
        "#Estimating SHAP values for our example\n",
        "shap_values = explainer([eg_comedy])\n",
        "\n",
        "shap.plots.text(shap_values)"
      ],
      "metadata": {
        "id": "mQ_SAYyzQEwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creaing an explainer that wraps around our pipeline\n",
        "explainer = shap.Explainer(pred)\n",
        "\n",
        "#Estimating SHAP values for our example\n",
        "shap_values = explainer([eg_horror])\n",
        "\n",
        "shap.plots.text(shap_values)"
      ],
      "metadata": {
        "id": "erc3eLyDQ6wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creaing an explainer that wraps around our pipeline\n",
        "explainer = shap.Explainer(pred)\n",
        "\n",
        "made_up_horror_plot = 'A lovely couple is lost in the woods, trying to escape from zombies.'\n",
        "made_up_comedy_plot = 'A lovely couple is lost in the woods, trying to escape from their wedding.'\n",
        "\n",
        "\n",
        "#Estimating SHAP values for our example\n",
        "shap_values = explainer([made_up_horror_plot, made_up_comedy_plot])\n",
        "\n",
        "shap.plots.text(shap_values)"
      ],
      "metadata": {
        "id": "R1aDLFHXRLCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explanations = 'For each input, for both existing plots and short made-up ones, model is able to pick up horror and comedy themes and distinguish them very well. Werevolwes zombies seem to be associated with horror, for instance, while weddings and college are associated more with comedy movies. These discriminations surely improve the model performance.' #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "QO9Ws1HvZWzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1d)** Build a SHAP-based explainer of your model over a substantial subsample of your data. Describe the aggregate feature importance values you obtain for each of your labels.\n",
        "\n",
        "NOTE: This task may not be appropriate for models other than for sequence classification."
      ],
      "metadata": {
        "id": "RUXp_0TNbWmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Estimating SHAP values for first three text inputs\n",
        "small_plots = wiki[wiki['len'] < 200]\n",
        "shap_values = explainer(small_plots[small_plots['label']==0].sample(n=100, random_state=1)['Plot'])"
      ],
      "metadata": {
        "id": "85g6wedtTfC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.bar(shap_values[:,:,\"LABEL_0\"].mean(0))"
      ],
      "metadata": {
        "id": "rKeHxxeVUJKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.bar(shap_values[:,:,\"LABEL_1\"].mean(0))"
      ],
      "metadata": {
        "id": "bR7OeZ7SV-Vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explanations = 'We can easily notice that the model gives the most weight to tokens traditionally associated with thriller and horror movies.' #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "ce9G17n5cQZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpTIL2NbL5TB"
      },
      "source": [
        "**2a)** Use a different large pre-trained language model for sequence classication, question answering, language modelling, or any other NLP task. Fine-tune the model on your dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FINzXTJBL_Ft"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ-NR_slMBMO"
      },
      "source": [
        "**2b)** What model did you use? How did your model perform?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtMzWp1uMCzV"
      },
      "source": [
        "model_performance = 'something' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qSjegARMEms"
      },
      "source": [
        "**3a)** Use BERT or a related model to create word and sentence embeddings from your corpus, and perform different similarity metrics on your embeddings, as well as perplexity measures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRwBdNBXMjPd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPN6WV1eMjkB"
      },
      "source": [
        "**3b)** How does fine-tuning your model change the similarity and perplexity metrics?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1os9EzltMn9Y"
      },
      "source": [
        "fine_tuning = 'something' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZoyb9z3NADe"
      },
      "source": [
        "**4)** BONUS: Fine tune a Transformers based model and use bertviz to visualise its attention layers using two sentences. Compare this to two sentences on a non fine tuned model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlbGm4W4NIqS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hGeVM6MMogG"
      },
      "source": [
        "**5)** BONUS: use a PyTorch or Keras sequence to sequence model to perform a task other than machine translation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euMHNChbMyT-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}